{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n==================\nENN, RENN, All-KNN\n==================\n\nAn illustration of the ENN, RENN, and All-KNN method.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Dayvid Oliveira\n#          Christos Aridas\n#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\n\nfrom imblearn.under_sampling import (AllKNN, EditedNearestNeighbours,\n                                     RepeatedEditedNearestNeighbours)\n\nprint(__doc__)\n\n\ndef plot_resampling(ax, X, y, title):\n    c0 = ax.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5)\n    c1 = ax.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5)\n    ax.set_title(title)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    ax.set_xlim([-6, 8])\n    ax.set_ylim([-6, 6])\n\n    return c0, c1\n\n\n# Generate the dataset\nX, y = make_classification(n_classes=2, class_sep=0.4, weights=[0.4, 0.6],\n                           n_informative=3, n_redundant=1, flip_y=0,\n                           n_features=5, n_clusters_per_class=1,\n                           n_samples=100, random_state=10)\n\n# Instanciate a PCA object for the sake of easy visualisation\npca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nX_vis = pca.fit_transform(X)\n\n# Three subplots, unpack the axes array immediately\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\nc0, c1 = plot_resampling(ax1, X_vis, y, 'Original set')\n\n# Apply the ENN\nprint('ENN')\nenn = EditedNearestNeighbours(return_indices=True)\nX_resampled, y_resampled, idx_resampled = enn.fit_sample(X, y)\nX_res_vis = pca.transform(X_resampled)\nidx_samples_removed = np.setdiff1d(np.arange(X_vis.shape[0]), idx_resampled)\nreduction_str = ('Reduced {:.2f}%'.format(100 * (1 - float(len(X_resampled)) /\n                                                 len(X))))\nprint(reduction_str)\nc3 = ax2.scatter(X_vis[idx_samples_removed, 0],\n                 X_vis[idx_samples_removed, 1],\n                 alpha=.2, label='Removed samples', c='g')\nplot_resampling(ax2, X_res_vis, y_resampled, 'ENN - ' + reduction_str)\n\n# Apply the RENN\nprint('RENN')\nrenn = RepeatedEditedNearestNeighbours(return_indices=True)\nX_resampled, y_resampled, idx_resampled = renn.fit_sample(X, y)\nX_res_vis = pca.transform(X_resampled)\nidx_samples_removed = np.setdiff1d(np.arange(X_vis.shape[0]), idx_resampled)\nreduction_str = ('Reduced {:.2f}%'.format(100 * (1 - float(len(X_resampled)) /\n                                                 len(X))))\nprint(reduction_str)\nax3.scatter(X_vis[idx_samples_removed, 0],\n            X_vis[idx_samples_removed, 1],\n            alpha=.2, label='Removed samples', c='g')\nplot_resampling(ax3, X_res_vis, y_resampled, 'RENN - ' + reduction_str)\n\n# Apply the AllKNN\nprint('AllKNN')\nallknn = AllKNN(return_indices=True)\nX_resampled, y_resampled, idx_resampled = allknn.fit_sample(X, y)\nX_res_vis = pca.transform(X_resampled)\nidx_samples_removed = np.setdiff1d(np.arange(X_vis.shape[0]), idx_resampled)\nreduction_str = ('Reduced {:.2f}%'.format(100 * (1 - float(len(X_resampled)) /\n                                                 len(X))))\nprint(reduction_str)\nax4.scatter(X_vis[idx_samples_removed, 0],\n            X_vis[idx_samples_removed, 1],\n            alpha=.2, label='Removed samples', c='g')\nplot_resampling(ax4, X_res_vis, y_resampled, 'All-KNN - ' + reduction_str)\n\nplt.figlegend((c0, c1, c3), ('Class #0', 'Class #1', 'Removed samples'),\n              loc='lower center', ncol=3, labelspacing=0.)\nplt.tight_layout(pad=3)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}