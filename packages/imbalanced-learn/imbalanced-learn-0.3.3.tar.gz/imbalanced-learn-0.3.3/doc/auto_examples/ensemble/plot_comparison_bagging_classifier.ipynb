{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of balanced and imbalanced bagging classifiers\n\n\nThis example shows the benefit of balancing the training set when using a\nbagging classifier. ``BalancedBaggingClassifier`` chains a\n``RandomUnderSampler`` and a given classifier while ``BaggingClassifier`` is\nusing directly the imbalanced data.\n\nBalancing the data set before training the classifier improve the\nclassification performance. In addition, it avoids the ensemble to focus on the\nmajority class which would be a known drawback of the decision tree\nclassifiers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\nimport itertools\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\n\nfrom imblearn.datasets import fetch_datasets\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfrom imblearn.metrics import classification_report_imbalanced\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nozone = fetch_datasets()['ozone_level']\nX, y = ozone.data, ozone.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nbagging = BaggingClassifier(random_state=0)\nbalanced_bagging = BalancedBaggingClassifier(random_state=0)\n\nprint('Class distribution of the training set: {}'.format(Counter(y_train)))\n\nbagging.fit(X_train, y_train)\nbalanced_bagging.fit(X_train, y_train)\n\nprint('Class distribution of the test set: {}'.format(Counter(y_test)))\n\nprint('Classification results using a bagging classifier on imbalanced data')\ny_pred_bagging = bagging.predict(X_test)\nprint(classification_report_imbalanced(y_test, y_pred_bagging))\ncm_bagging = confusion_matrix(y_test, y_pred_bagging)\nplt.figure()\nplot_confusion_matrix(cm_bagging, classes=np.unique(ozone.target),\n                      title='Confusion matrix using BaggingClassifier')\n\nprint('Classification results using a bagging classifier on balanced data')\ny_pred_balanced_bagging = balanced_bagging.predict(X_test)\nprint(classification_report_imbalanced(y_test, y_pred_balanced_bagging))\ncm_balanced_bagging = confusion_matrix(y_test, y_pred_balanced_bagging)\nplt.figure()\nplot_confusion_matrix(cm_balanced_bagging, classes=np.unique(ozone.target),\n                      title='Confusion matrix using BalancedBaggingClassifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Turning the balanced bagging classifier into a balanced random forest\n##############################################################################\n It is possible to turn the ``BalancedBaggingClassifier`` into a balanced\n random forest by using a ``DecisionTreeClassifier`` with\n ``max_features='auto'``. We illustrate such changes below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "balanced_random_forest = BalancedBaggingClassifier(\n    base_estimator=DecisionTreeClassifier(max_features='auto'),\n    random_state=0)\n\nbalanced_random_forest.fit(X_train, y_train)\nprint('Classification results using a balanced random forest classifier on'\n      ' imbalanced data')\ny_pred_balanced_rf = balanced_random_forest.predict(X_test)\nprint(classification_report_imbalanced(y_test, y_pred_balanced_rf))\ncm_bagging = confusion_matrix(y_test, y_pred_balanced_rf)\nplt.figure()\nplot_confusion_matrix(cm_bagging, classes=np.unique(ozone.target),\n                      title='Confusion matrix using balanced random forest')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}