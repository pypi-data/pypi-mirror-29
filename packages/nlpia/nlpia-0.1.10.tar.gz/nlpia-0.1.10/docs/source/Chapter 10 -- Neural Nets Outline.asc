Neural Nets
(Deep Learning)

Word2Vec

Relations
CNN

Memory
RNN
LSTM
(LSTM shortcut model)
Neural Turing Models

Tools:
- Theano
- Tensorflow
- Keras


What to do with them?

- Character Level
    - Nitzche bot
    - This will pursue 'voice' and 'style' but provide no control of content.  How would one combine this with other techniques .  To apply this style to content.
    - Perhaps there is something to be found in selecting the training set based on content AND style (though this would demand many pretrained models, or training on demand which would be time prohibitive for a user)
- Word Level (seq2seq)
    - Translation
    - Movie Scripts
    - Now we have one bit of context, and a specific response can be generated.  What defines the context of a response?  Is there something inherent in text that makes something responsive (even if a real question isn't specifically answered)  This would be something fun to come up with hard, human-generated examples of nonsense responses that could easily be considered viable if not correct.  And then compare that with the response of the movie-dialog bot.
- Tweet Level
- Document Level

- Parsing

- Generation

- Validation after the fact:
    - Match style
    - Match sentiment
    - Choose to not do those (for what reason?)
        - Human nature to continue user engagement

Nitty Gritty

- Hyperparmeter Optimization
- Distributed learning
    - Spark
    - Google
    - Kubernetes


Non Neural Net notes:

- Textblob (Python Library)
